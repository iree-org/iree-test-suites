from abc import ABC, abstractmethod
from numbers import Number
from typing import Tuple
import numpy as np
from pathlib import Path
from dataclasses import dataclass
import json
import functools
import pytest

import torch
import iree.turbine.aot as aot


def camel_to_snake(name):
    result = ""
    for i, char in enumerate(name):
        if char.isupper() and i > 0:
            result += "_"
        result += char.lower()
    return result

def single_input(coeff: Number, shape: Tuple[int, ...], offset: Number, dtype):
    tensor = coeff * np.random.rand(*shape) + offset
    tensor = tensor.astype(dtype)
    return torch.from_numpy(tensor)

def many_inputs(list_of_params):
    return [single_input(*params) for params in list_of_params]

def generate_inputs(list_of_params, seed):
    np.random.seed(seed)
    return many_inputs(list_of_params)

class QualityTestGenerator(torch.nn.Module, ABC):
    """
    Class used to generate correctness tests
    for `torch.nn.Module`s. These tests are MLIR modules that have
    been exported and their inputs and expected outputs have been saved.
    Some other metadata may be used for tests as well like tolerance for
    floating point comparisons.

    Example:

    ```python
    # Instead of using torch.nn.Module
    # as a base class for a torch Module,
    # use this class instead.
    class MyTest(QualityTestGenerator):

      def forward(self, left, right):
        return left @ right

      # Use the test decorator to denote a test.
      @test
      def test_16x16(self):
        # Use self.rand instead of torch.rand.
        # It is a wrapper around torch.rand to make sure tests are deterministic
        # and seeds are easily specified

        # The output is a tuple of (Sequence, dict)
        # that corresponds to *args, **kwargs
        return ((self.rand(16, 16), self.rand(16, 16), {})

      # Add more tests by adding a new decorator
      @test(seed=42, atol=1e-7, rtol=1e-9)
      def test_8x8(self):
        return ((self.rand(8, 8), self.rand(8, 8), {})

    # Will create a directory with the test and metadata needed to execute
    # the test.
    MyTest().generate_tests()
    ```
    """

    def __init__(self):
        self.test_config = {}
        self.generator = None
        super().__init__()

    def get_export_kwargs(self):
        """Default implementation returns an empty dictionary.

        This dictionary will be passed to IREE's aot.export.
        See here for aot.export's documentation on possible kwargs
        https://github.com/iree-org/iree-turbine/blob/fabcbad626860d190623d1a1ee432efcf9174315/iree/turbine/aot/exporter.py#L228-L233
        and here for torch.export's documentation
        https://docs.pytorch.org/docs/stable/export/api_reference.html
        """
        return {}

    def save_mlir(self, path, *args):
        """Save MLIR file into $path/test.mlir.

        export_kwargs from self.get_export_kwargs are used when exporting the module.
        """
        export_kwargs = self.get_export_kwargs()
        exported_module = aot.export(self, *args, **export_kwargs)
        exported_module.save_mlir(path / "test.mlir")

    def save_inputs(self, root_path, test_config, *args):
        """Save inputs into a ${root_path}/input${idx}.npy
        and updates test_config to denote number inputs to this test.
        """
        inputs = []
        for idx, input in enumerate(args):
            fname = f"input{idx}.npy"
            path = root_path / fname
            np.save(path, input)
            inputs.append(fname)
        test_config["inputs"] = inputs
        return test_config

    def save_results(self, root_path, test_config, *args):
        """Save expected output to ${root_path}/expected_result${idx}.npy
        and updates test_config with expected results and path to observed
        results files which will then be generated by the test itself."""
        expected_outputs = []
        observed_outputs = []
        for idx, result in enumerate(args):
            fname_expected = f"expected_result{idx}.npy"
            fname_observed = f"observed_result{idx}.npy"
            path = root_path / fname_expected
            np.save(path, result)
            expected_outputs.append(fname_expected)
            observed_outputs.append(fname_observed)
        test_config["expected_outputs"] = expected_outputs
        test_config["observed_outputs"] = observed_outputs
        return test_config

    def save_config(self, root_path, test_config):
        """Save configuration file."""
        with open(root_path / "run_module_io_flags.json", "w") as config:
            json.dump(test_config, config, indent=4)
            print("", file=config)

    def generate_tests(self):
        """For each test method run the torch.nn.Module to generate an expected result
        and then:
            * save MLIR module
            * save inputs
            * save expected outputs
            * save configuration file
        """
        for name in sorted(dir(self)):
            is_test = name.startswith("test_")
            is_benchmark = name.startswith("benchmark_")
            if not (is_test or is_benchmark):
                continue

            attr = getattr(self, name)
            if not callable(attr):
                continue

            # This is for when the class was annotated
            # with pytest.mark
            markers = [] if not hasattr(self, "pytestmark") else self.pytestmark
            if hasattr(attr, "pytestmark"):
                # This is for when the function was annotated with
                # pytest.mark
                markers = attr.pytestmark

            markers_dict = {}
            for marker in markers:
                markers_dict.update(
                    {marker.name: {"args": marker.args, "kwargs": marker.kwargs}}
                )

            test_config = {"markers": markers_dict}
            if is_test:
                pathname = f"test_{camel_to_snake(self._get_name())}_{name[5:]}"
            elif is_benchmark:
                pathname = f"benchmark_{camel_to_snake(self._get_name())}_{name[10:]}"

            path = "generated" / Path(pathname)
            path.mkdir(parents=True, exist_ok=True)
            inputs_to_forward = attr

            if is_test:
                args, kwargs, test_kwargs = inputs_to_forward()
                test_config["rtol"] = test_kwargs["rtol"]
                test_config["atol"] = test_kwargs["atol"]
                test_config["equal_nan"] = test_kwargs["equal_nan"]

            elif is_benchmark:
                seed, parameter_list = inputs_to_forward()
                test_config["param_list"] = parameter_list
                args = generate_inputs(parameter_list, seed)
                kwargs = {}

            self.save_mlir(path, *args, **kwargs)

            if is_test:
                expected_results = self.generate_expected_value(*args, **kwargs)
                test_config = self.save_inputs(path, test_config, *args)
                test_config = self.save_results(path, test_config, *expected_results)


            self.save_config(path, test_config)

    def generate_expected_value(self, *args):
        """Call forward"""
        results = self.forward(*args)
        return [results]

    @abstractmethod
    def forward(self, *args):
        ...

    def rand(self, *args, **kwargs):
        """Wrapper around torch.rand

        The main purpose of this wrapper is to enforce that the generator argument
        is passed through the test decorator. This way, each test is deterministic.
        """
        if kwargs.get("generator"):
            raise ValueError("Set generator with parameter to test function.")
        kwargs["generator"] = self.generator
        return torch.rand(*args, **kwargs)


# TODO(@amd-eochoalo): I think this can be made into a pytest.mark.correctness
# Also, I think one could do something like:
#
# - /generated/${model}/
# - /generated/${model}/model.mlir
# - /generated/${model}/test_${inputs}/
# - /generated/${model}/test_${inputs}/config.json
#
# And have config.json be the PyTestItem.
# That way we can have a single mlir file be shared among multiple
# different inputs.
def test(
    function=None, generator=None, seed=0, rtol=1e-05, atol=1e-08, equal_nan=False
):
    """
    Decorator used to denote that a particular method corresponds
    to test input used in the forward method.

    The default values for rtol, atol, and equal_nan come from the
    numpy.allclose's default values listed in the link below.
    https://numpy.org/devdocs/reference/generated/numpy.allclose.html

    The generator will be seeded with seed and later be passed on
    to a torch.rand wrapper.

    These values will be saved in the json file and be used when
    running the test.

    The name of the function must start with "test_"
    """

    if not generator:
        generator = torch.Generator()

    test_kwargs = {"rtol": rtol, "atol": atol, "equal_nan": equal_nan}

    if function:
        if not function.__name__.startswith("test_"):
            raise ValueError("The name of test functions should start with test_")

        @functools.wraps(function)
        def wrapper(self):
            self.generator = generator
            self.generator.manual_seed(seed)
            args, kwargs = function(self)
            self.generator = None
            return args, kwargs, test_kwargs

        return wrapper

    return functools.partial(test, **test_kwargs)

def benchmark(function=None, seed=0):
    """
    Decorator used to denote that a particular method corresponds
    to a benchmark input.

    Args:
      function (Callable[Self, Sequence[Tuple[Number, Tuple[int,...], Number, dtype):
            To explain this function it is important to explain that we would
            like to generate a random number with some properties.
            So, all random numbers generated by the test runner will be of the
            generated with:

            ```
            def single_input(coeff: Number, shape: Tuple[int, ...], offset: Number, dtype):
                tensor = coeff * numpy.random.rand(*shape) + offset
                tensor = tensor.astype(dtype)
                return tensor
            ```

            This means that this function is expected to return all the input parameters
            to the `single_input` function above. However, we want to be able to generate
            multiple inputs in sequence, not just a single_input. So this function should
            return a Sequence of the parameter types to the single_input function.

            ```
            def many_inputs(list_of_params):
                return [single_input(*params) for params in list_of_params]
            ```

      seed (int): Seed used as a parameter to numpy.random.seed before
                  generating inputs from function.
            ```
            def generate_inputs(list_of_params, seed):
                numpy.random.seed(seed)
                return many_inputs(list_of_params)
            ```

    These values will be saved in the json file and be used when
    running the test.

    The name of the function must start with "benchmark_"
    """
    if function:
        if not function.__name__.startswith("benchmark_"):
            raise ValueError("The name of benchmark functions should start with benchmark_")

        @functools.wraps(function)
        def wrapper(self):
            param_list = function(self)
            return seed, param_list
        return wrapper

    benchmark_kwargs = {"seed": seed}

    return functools.partial(benchmark, **benchmark_kwargs)


@pytest.mark.benchmark
@pytest.mark.correctness
class AB(QualityTestGenerator):
    def forward(self, left, right):
        return left @ right

    @test(seed=0)
    def test_float32(self):
        left = self.rand(64, 64, dtype=torch.float32)
        right = self.rand(64, 64, dtype=torch.float32)
        return ((left, right), {})

    @test(seed=1)
    def test_float16(self):
        left = self.rand(64, 64, dtype=torch.float16)
        right = self.rand(64, 64, dtype=torch.float16)
        return ((left, right), {})

    @benchmark(seed=0)
    def benchmark_float32(self):
        return [(1, (64, 64), 0, "float32")] * 2

    def get_export_kwargs(self):
        dyn_dim = torch.export.Dim("N")
        dynamic_shapes = {"left": {0: dyn_dim}, "right": {1: dyn_dim}}
        return {"dynamic_shapes": dynamic_shapes}


@pytest.mark.correctness
class AB_bfloat16(QualityTestGenerator):
    def forward(self, left, right):
        left = left.to(torch.bfloat16)
        right = right.to(torch.bfloat16)
        res = left @ right
        return res.to(torch.float32)

    @test(atol=1e-2, rtol=1e-2, seed=2)
    def test_from_float32(self):
        left = self.rand(64, 64, dtype=torch.float32)
        right = self.rand(64, 64, dtype=torch.float32)
        return ((left, right), {})

    def get_export_kwargs(self):
        dyn_dim = torch.export.Dim("N")
        dynamic_shapes = {"left": {0: dyn_dim}, "right": {1: dyn_dim}}
        return {"dynamic_shapes": dynamic_shapes}


@pytest.mark.correctness
class ATB(QualityTestGenerator):
    def forward(self, left, right):
        return left.t() @ right

    @test(seed=3)
    def test_float32(self):
        left = self.rand(64, 64, dtype=torch.float32)
        right = self.rand(64, 64, dtype=torch.float32)
        return ((left, right), {})

    @test(seed=4)
    def test_float16(self):
        left = self.rand(64, 64, dtype=torch.float16)
        right = self.rand(64, 64, dtype=torch.float16)
        return ((left, right), {})


@pytest.mark.correctness
class ABT(QualityTestGenerator):
    def forward(self, left, right):
        return left @ right.t()

    @test(seed=5)
    def test_float32(self):
        left = self.rand(64, 64, dtype=torch.float32)
        right = self.rand(64, 64, dtype=torch.float32)
        return ((left, right), {})

    @test(seed=6)
    def test_float16(self):
        left = self.rand(64, 64, dtype=torch.float16)
        right = self.rand(64, 64, dtype=torch.float16)
        return ((left, right), {})


@pytest.mark.correctness
class ABPlusC(QualityTestGenerator):
    def forward(self, A, B, C):
        return A @ B + C

    @test(seed=7)
    def test_float32(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
            ),
            {},
        )

    @test(seed=8)
    def test_float16(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
            ),
            {},
        )


@pytest.mark.correctness
class ReluABPlusC(QualityTestGenerator):
    def forward(self, A, B, C):
        return torch.relu(A @ B + C)

    @test(seed=9)
    def test_float32(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
                self.rand(64, 64, dtype=torch.float32) * 2 - 1,
            ),
            {},
        )

    @test(seed=10)
    def test_float16(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
                self.rand(64, 64, dtype=torch.float16) * 2 - 1,
            ),
            {},
        )


@pytest.mark.correctness
class GeluABPlusC(QualityTestGenerator):
    def forward(self, A, B, C):
        return torch.ops.aten.gelu.default(A @ B + C)

    @test(seed=11, atol=1e-5, rtol=1e-4)
    def test_float32(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
            ),
            {},
        )

    @test(seed=12, atol=1e-3, rtol=1e-3)
    def test_float16(self):
        return (
            (
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
                self.rand(64, 64, dtype=torch.float32) * 0.1 - 0.05,
            ),
            {},
        )


for cls in [AB, AB_bfloat16, ATB, ABT, ABPlusC, ReluABPlusC, GeluABPlusC]:
    instance = cls()
    instance.generate_tests()
